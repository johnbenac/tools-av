#!/usr/bin/env python3
"""
av-editor - Multi-source timeline editor with clapboard sync

Synchronizes video and audio from multiple sources using clapboard
timestamps, with per-source timelines for z-index based compositing.

New in 0.3.x:
- production.includes (required) lets you keep only specific master-timeline ranges,
  concatenating them into the final output.
"""

import argparse
import json
import os
import re
import subprocess
import sys
import tempfile
from pathlib import Path
from typing import Any, Dict, List, Tuple

VERSION = "0.3.0"


class AVEditorError(Exception):
    """Base exception for av-editor errors"""
    pass


def ffmpeg_has_encoder(encoder_name):
    """Check whether ffmpeg supports a specific encoder."""
    output = run_command(['ffmpeg', '-hide_banner', '-encoders'])
    return f" {encoder_name} " in output


def run_command(cmd, verbose=False, dry_run=False):
    """Run a command and return output"""
    if verbose or dry_run:
        # Format command for readability
        cmd_str = ' '.join(cmd)
        if len(cmd_str) > 120:
            print(f"[CMD] {cmd_str[:120]}...")
        else:
            print(f"[CMD] {cmd_str}")

    if dry_run:
        return None

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True
        )
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        raise AVEditorError(f"Command failed: {' '.join(cmd)}\n{e.stderr}")


def probe_duration(file_path, stream_type=None, verbose=False):
    """Get duration of a file, trying stream duration first then format"""
    if stream_type:
        cmd = [
            'ffprobe', '-v', 'error',
            '-select_streams', f'{stream_type}:0',
            '-show_entries', 'stream=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            str(file_path)
        ]
        try:
            output = run_command(cmd)
            if output and output != 'N/A':
                dur = float(output)
                if dur > 0:
                    if verbose:
                        print(f"[INFO] {Path(file_path).name}: {stream_type} stream duration = {dur:.3f}s")
                    return dur
        except (ValueError, AVEditorError):
            pass

    # Fallback to format duration
    cmd = [
        'ffprobe', '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        str(file_path)
    ]
    output = run_command(cmd)
    if output and output != 'N/A':
        dur = float(output)
        if verbose:
            print(f"[INFO] {Path(file_path).name}: format duration = {dur:.3f}s")
        return dur

    raise AVEditorError(f"Could not determine duration for {file_path}")


def probe_resolution(file_path, verbose=False):
    """Get video resolution (width, height)"""
    cmd = [
        'ffprobe', '-v', 'error',
        '-select_streams', 'v:0',
        '-show_entries', 'stream=width,height',
        '-of', 'json',
        str(file_path)
    ]
    output = run_command(cmd)
    data = json.loads(output)
    if 'streams' in data and data['streams']:
        stream = data['streams'][0]
        width = stream.get('width')
        height = stream.get('height')
        if width and height:
            return (width, height)

    raise AVEditorError(f"Could not determine resolution for {file_path}")


def validate_file(file_path):
    """Check file exists and is readable"""
    p = Path(file_path)
    if not p.exists():
        raise AVEditorError(f"File not found: {file_path}")
    if not p.is_file():
        raise AVEditorError(f"Not a file: {file_path}")
    if not os.access(p, os.R_OK):
        raise AVEditorError(f"File not readable: {file_path}")


def _is_numberish(x: Any) -> bool:
    return isinstance(x, (int, float)) or (isinstance(x, str) and x.strip() != "")


def _to_float(x: Any, ctx: str) -> float:
    if not _is_numberish(x):
        raise AVEditorError(f"{ctx} must be a number, got {type(x).__name__}")
    try:
        return float(x)
    except ValueError as e:
        raise AVEditorError(f"{ctx} must be a number, got {x!r}") from e


def _parse_position(value: Any, ctx: str) -> Tuple[float, float]:
    if not (isinstance(value, list) and len(value) == 2):
        raise AVEditorError(f"{ctx} must be [x_percent, y_percent]")
    x_pct = _to_float(value[0], f"{ctx}[0]")
    y_pct = _to_float(value[1], f"{ctx}[1]")
    # Allow off-canvas placement (negative or >100) for partial framing workflows.
    return (x_pct, y_pct)


def _parse_scale(value: Any, ctx: str) -> float:
    scale_pct = _to_float(value, ctx)
    if scale_pct <= 0.0:
        raise AVEditorError(f"{ctx} must be > 0")
    return scale_pct


def _parse_crop(value: Any, ctx: str) -> Tuple[float, float, float, float]:
    """Parse crop percentages [left, right, top, bottom]."""
    if not (isinstance(value, list) and len(value) == 4):
        raise AVEditorError(f"{ctx} must be [left, right, top, bottom]")

    left_pct = _to_float(value[0], f"{ctx}[0]")
    right_pct = _to_float(value[1], f"{ctx}[1]")
    top_pct = _to_float(value[2], f"{ctx}[2]")
    bottom_pct = _to_float(value[3], f"{ctx}[3]")

    for idx, v in enumerate((left_pct, right_pct, top_pct, bottom_pct)):
        if not (0.0 <= v <= 100.0):
            raise AVEditorError(f"{ctx}[{idx}] must be between 0 and 100")

    if left_pct + right_pct >= 100.0:
        raise AVEditorError(f"{ctx} left+right must be < 100")
    if top_pct + bottom_pct >= 100.0:
        raise AVEditorError(f"{ctx} top+bottom must be < 100")

    return (left_pct, right_pct, top_pct, bottom_pct)


def _safe_label(text: str) -> str:
    """Make a string safe for ffmpeg filter labels."""
    return re.sub(r'[^0-9A-Za-z_]', '_', text)


def _validate_includes_object(includes: Any) -> None:
    """
    Validate production.includes structure.

    Required shape:
      includes: {}  (empty -> include everything)
      or:
      includes: {
        "label": [start, end],
        "label2": [[start, end], [start, end]]
      }

    Notes:
    - JSON objects can't have duplicate keys. If you want multiple ranges with the same
      label, use a list-of-ranges under that label.
    """
    if not isinstance(includes, dict):
        raise AVEditorError("production.includes must be an object (e.g. {} or {\"label\": [start,end]})")

    for label, spec in includes.items():
        # spec can be [start,end] OR [[start,end], ...]
        if not isinstance(spec, list):
            raise AVEditorError(f"production.includes.{label} must be an array")

        if len(spec) == 2 and not any(isinstance(x, list) for x in spec):
            # [start,end]
            _to_float(spec[0], f"production.includes.{label}[0]")
            _to_float(spec[1], f"production.includes.{label}[1]")
            continue

        # list-of-ranges
        if not spec:
            raise AVEditorError(f"production.includes.{label} cannot be an empty array (use {{}} for include-all)")
        for i, pair in enumerate(spec):
            if not (isinstance(pair, list) and len(pair) == 2):
                raise AVEditorError(f"production.includes.{label}[{i}] must be [start, end]")
            _to_float(pair[0], f"production.includes.{label}[{i}][0]")
            _to_float(pair[1], f"production.includes.{label}[{i}][1]")


def load_config(config_path):
    """Load and validate JSON config file"""
    validate_file(config_path)

    with open(config_path, 'r') as f:
        try:
            config = json.load(f)
        except json.JSONDecodeError as e:
            raise AVEditorError(f"Invalid JSON in {config_path}: {e}")

    # Validate required fields
    if 'master_audio' not in config:
        raise AVEditorError("Config missing 'master_audio' section")

    ma = config['master_audio']
    if 'file' not in ma:
        raise AVEditorError("master_audio missing 'file'")
    if 'clap_time' not in ma:
        raise AVEditorError("master_audio missing 'clap_time'")

    if 'video_sources' not in config or not config['video_sources']:
        raise AVEditorError("Config missing 'video_sources' (need at least one)")

    # Validate video_sources is a dict with named sources
    if not isinstance(config['video_sources'], dict):
        raise AVEditorError("video_sources must be an object with named sources")

    for name, vs in config['video_sources'].items():
        if 'file' not in vs:
            raise AVEditorError(f"video_sources.{name} missing 'file'")
        if 'clap_time' not in vs:
            raise AVEditorError(f"video_sources.{name} missing 'clap_time'")
        if 'z_index' not in vs:
            raise AVEditorError(f"video_sources.{name} missing 'z_index'")
        _to_float(vs['z_index'], f"video_sources.{name}.z_index")

        # Optional placement/crop controls:
        # - position: [x_percent, y_percent] for top-left corner
        # - scale: percent of output size (100 means full frame)
        # - crop: [left, right, top, bottom] in percent of source frame
        if 'position' in vs:
            _parse_position(vs['position'], f"video_sources.{name}.position")

        if 'scale' in vs:
            _parse_scale(vs['scale'], f"video_sources.{name}.scale")

        if 'crop' in vs:
            _parse_crop(vs['crop'], f"video_sources.{name}.crop")

        # Validate timeline if present
        if 'timeline' in vs:
            if not isinstance(vs['timeline'], list):
                raise AVEditorError(f"video_sources.{name}.timeline must be an array")
            for i, event in enumerate(vs['timeline']):
                if not isinstance(event, dict):
                    raise AVEditorError(f"video_sources.{name}.timeline[{i}] must be an object")
                if 'at' not in event:
                    raise AVEditorError(f"video_sources.{name}.timeline[{i}] missing 'at'")
                _to_float(event['at'], f"video_sources.{name}.timeline[{i}].at")

                has_change = False
                if 'z_index' in event:
                    _to_float(event['z_index'], f"video_sources.{name}.timeline[{i}].z_index")
                    has_change = True
                if 'position' in event:
                    _parse_position(event['position'], f"video_sources.{name}.timeline[{i}].position")
                    has_change = True
                if 'scale' in event:
                    _parse_scale(event['scale'], f"video_sources.{name}.timeline[{i}].scale")
                    has_change = True
                if 'crop' in event:
                    _parse_crop(event['crop'], f"video_sources.{name}.timeline[{i}].crop")
                    has_change = True

                if not has_change:
                    raise AVEditorError(
                        f"video_sources.{name}.timeline[{i}] must include at least one of "
                        f"'z_index', 'position', 'scale', or 'crop'")

    if 'production' not in config:
        raise AVEditorError("Config missing 'production' section")

    prod = config['production']
    if 'output_file' not in prod:
        raise AVEditorError("production missing 'output_file'")
    if 'start' not in prod:
        raise AVEditorError("production missing 'start'")
    if 'end' not in prod:
        raise AVEditorError("production missing 'end'")

    # NEW: production.includes is required (can be empty {})
    if 'includes' not in prod:
        raise AVEditorError("production missing required 'includes' (use {} to include everything)")
    _validate_includes_object(prod['includes'])

    return config


def build_source_state_timeline(sources, production_start, production_duration):
    """
    Build a complete per-source state timeline for z-index and placement.

    Timeline events are absolute master-audio times and may update any subset
    of: z_index, position, scale, crop.

    Returns a list of:
      (timestamp_in_segment, {
          source_name: {
              'z_index': float,
              'position_x_pct': float,
              'position_y_pct': float,
              'scale_pct': float,
              'crop_left_pct': float,
              'crop_right_pct': float,
              'crop_top_pct': float,
              'crop_bottom_pct': float
          }
      })
    """
    production_end = production_start + production_duration

    # For each source, build a timeline of partial state updates in absolute master time.
    absolute_timelines: Dict[str, List[Dict[str, float]]] = {}
    default_states: Dict[str, Dict[str, float]] = {}
    for name, src in sources.items():
        pos_x_pct, pos_y_pct = _parse_position(src.get('position', [0.0, 0.0]),
                                               f"video_sources.{name}.position")
        scale_pct = _parse_scale(src.get('scale', 100.0), f"video_sources.{name}.scale")
        crop_left_pct, crop_right_pct, crop_top_pct, crop_bottom_pct = _parse_crop(
            src.get('crop', [0.0, 0.0, 0.0, 0.0]),
            f"video_sources.{name}.crop"
        )
        default_state = {
            'z_index': _to_float(src['z_index'], f"video_sources.{name}.z_index"),
            'position_x_pct': pos_x_pct,
            'position_y_pct': pos_y_pct,
            'scale_pct': scale_pct,
            'crop_left_pct': crop_left_pct,
            'crop_right_pct': crop_right_pct,
            'crop_top_pct': crop_top_pct,
            'crop_bottom_pct': crop_bottom_pct
        }
        default_states[name] = dict(default_state)

        events: List[Dict[str, float]] = [{'at': 0.0, **default_state}]
        if 'timeline' in src:
            for i, event in enumerate(src['timeline']):
                absolute_time = _to_float(event['at'], f"video_sources.{name}.timeline[{i}].at")
                event_update: Dict[str, float] = {'at': absolute_time}

                if 'z_index' in event:
                    event_update['z_index'] = _to_float(
                        event['z_index'],
                        f"video_sources.{name}.timeline[{i}].z_index"
                    )
                if 'position' in event:
                    event_pos_x_pct, event_pos_y_pct = _parse_position(
                        event['position'],
                        f"video_sources.{name}.timeline[{i}].position"
                    )
                    event_update['position_x_pct'] = event_pos_x_pct
                    event_update['position_y_pct'] = event_pos_y_pct
                if 'scale' in event:
                    event_update['scale_pct'] = _parse_scale(
                        event['scale'],
                        f"video_sources.{name}.timeline[{i}].scale"
                    )
                if 'crop' in event:
                    event_crop_left_pct, event_crop_right_pct, event_crop_top_pct, event_crop_bottom_pct = _parse_crop(
                        event['crop'],
                        f"video_sources.{name}.timeline[{i}].crop"
                    )
                    event_update['crop_left_pct'] = event_crop_left_pct
                    event_update['crop_right_pct'] = event_crop_right_pct
                    event_update['crop_top_pct'] = event_crop_top_pct
                    event_update['crop_bottom_pct'] = event_crop_bottom_pct

                events.append(event_update)

        events.sort(key=lambda e: e['at'])
        absolute_timelines[name] = events

    # Determine full source state at production start.
    initial_state: Dict[str, Dict[str, float]] = {}
    for name, events in absolute_timelines.items():
        current_state = dict(default_states[name])
        for event in events:
            timestamp = event['at']
            if timestamp <= production_start:
                for key in (
                    'z_index',
                    'position_x_pct',
                    'position_y_pct',
                    'scale_pct',
                    'crop_left_pct',
                    'crop_right_pct',
                    'crop_top_pct',
                    'crop_bottom_pct'
                ):
                    if key in event:
                        current_state[key] = event[key]
            else:
                break
        initial_state[name] = dict(current_state)

    # Collect all state updates that happen during production.
    production_events: List[Dict[str, Any]] = []
    for name, events in absolute_timelines.items():
        for event in events:
            timestamp = event['at']
            if production_start < timestamp <= production_end:
                changes: Dict[str, float] = {}
                for key in (
                    'z_index',
                    'position_x_pct',
                    'position_y_pct',
                    'scale_pct',
                    'crop_left_pct',
                    'crop_right_pct',
                    'crop_top_pct',
                    'crop_bottom_pct'
                ):
                    if key in event:
                        changes[key] = event[key]
                if changes:
                    production_events.append({
                        'at': timestamp - production_start,
                        'source': name,
                        'changes': changes
                    })

    production_events.sort(key=lambda e: e['at'])

    current_state: Dict[str, Dict[str, float]] = {
        name: dict(state) for name, state in initial_state.items()
    }
    timeline: List[Tuple[float, Dict[str, Dict[str, float]]]] = [
        (0.0, {name: dict(state) for name, state in current_state.items()})
    ]

    for event in production_events:
        source = event['source']
        for key, value in event['changes'].items():
            current_state[source][key] = value
        timeline.append((
            event['at'],
            {name: dict(state) for name, state in current_state.items()}
        ))

    return timeline


def parse_includes(includes: Dict[str, Any],
                   production_start: float,
                   production_end: float,
                   verbose: bool = False) -> List[Dict[str, Any]]:
    """
    Turn production.includes into a sorted list of included segments.

    - Empty {} => include full [production_start, production_end]
    - Non-empty => include union of the specified ranges (clipped to start/end window)

    Input format:
      includes: {
        "label": [start, end],
        "label2": [[start, end], [start, end]]
      }

    Times are in absolute master-audio seconds.
    Returns: [{'label': str|None, 'start': float, 'end': float}, ...]
    """
    if includes is None:
        raise AVEditorError("production.includes is required (use {} to include everything)")
    if not isinstance(includes, dict):
        raise AVEditorError("production.includes must be an object")

    if len(includes) == 0:
        return [{
            'label': None,
            'start': production_start,
            'end': production_end
        }]

    segs: List[Dict[str, Any]] = []
    for label, spec in includes.items():
        if not isinstance(spec, list):
            raise AVEditorError(f"production.includes.{label} must be an array")

        # [start,end]
        if len(spec) == 2 and not any(isinstance(x, list) for x in spec):
            s = _to_float(spec[0], f"production.includes.{label}[0]")
            e = _to_float(spec[1], f"production.includes.{label}[1]")
            segs.append({'label': label, 'start': s, 'end': e})
            continue

        # [[start,end], ...]
        for i, pair in enumerate(spec):
            if not (isinstance(pair, list) and len(pair) == 2):
                raise AVEditorError(f"production.includes.{label}[{i}] must be [start, end]")
            s = _to_float(pair[0], f"production.includes.{label}[{i}][0]")
            e = _to_float(pair[1], f"production.includes.{label}[{i}][1]")
            segs.append({'label': label, 'start': s, 'end': e})

    # Normalize: clip to production window, drop empty, sort
    normalized: List[Dict[str, Any]] = []
    for seg in segs:
        s = seg['start']
        e = seg['end']
        if e < s:
            raise AVEditorError(
                f"Invalid includes range for label {seg['label']!r}: start {s} > end {e}"
            )

        clipped_s = max(production_start, s)
        clipped_e = min(production_end, e)

        if clipped_e <= clipped_s:
            if verbose:
                print(f"[WARN] Dropping includes range {seg['label']!r} [{s:.3f}, {e:.3f}] "
                      f"(outside production window [{production_start:.3f}, {production_end:.3f}])")
            continue

        if (clipped_s != s or clipped_e != e) and verbose:
            print(f"[INFO] Clipped includes range {seg['label']!r} [{s:.3f}, {e:.3f}] "
                  f"-> [{clipped_s:.3f}, {clipped_e:.3f}] to fit production window")

        normalized.append({'label': seg['label'], 'start': clipped_s, 'end': clipped_e})

    if not normalized:
        raise AVEditorError("production.includes produced no usable ranges (all were empty/outside start..end)")

    normalized.sort(key=lambda x: (x['start'], x['end']))

    # Merge overlaps/touching ranges to avoid accidental duplicates (labels are informational only)
    merged: List[Dict[str, Any]] = []
    for seg in normalized:
        if not merged:
            merged.append(seg)
            continue
        prev = merged[-1]
        if seg['start'] <= prev['end']:
            if seg['end'] > prev['end']:
                prev['end'] = seg['end']
        else:
            merged.append(seg)

    return merged


def _state_segments_from_timeline(state_timeline: List[Tuple[float, Dict[str, Dict[str, float]]]],
                                  duration: float) -> List[Dict[str, Any]]:
    """Convert a state timeline into contiguous segments with constant source state."""
    segs: List[Dict[str, Any]] = []
    for i in range(len(state_timeline)):
        start_time = float(state_timeline[i][0])
        end_time = float(state_timeline[i + 1][0]) if i + 1 < len(state_timeline) else float(duration)
        if end_time <= start_time:
            continue
        segs.append({
            'start': start_time,
            'end': end_time,
            'state': {name: dict(props) for name, props in state_timeline[i][1].items()}
        })
    return segs


def render(args):
    """Render synchronized output from config"""
    config = load_config(args.config)
    verbose = args.verbose

    master = config['master_audio']
    master_file = master['file']
    master_clap = float(master['clap_time'])

    validate_file(master_file)
    master_duration = probe_duration(master_file, stream_type='a', verbose=verbose)

    print(f"Master audio: {master_file}")
    print(f"  Duration: {master_duration:.3f}s")
    print(f"  Clap at: {master_clap:.3f}s")

    # Process video sources
    video_sources = config['video_sources']
    source_info: Dict[str, Dict[str, Any]] = {}

    print(f"\nVideo sources:")
    for name, src in video_sources.items():
        file_path = src['file']
        clap_time = float(src['clap_time'])
        default_z = _to_float(src['z_index'], f"video_sources.{name}.z_index")
        pos_x_pct, pos_y_pct = _parse_position(
            src.get('position', [0.0, 0.0]),
            f"video_sources.{name}.position"
        )
        scale_pct = _parse_scale(src.get('scale', 100.0), f"video_sources.{name}.scale")
        crop_left_pct, crop_right_pct, crop_top_pct, crop_bottom_pct = _parse_crop(
            src.get('crop', [0.0, 0.0, 0.0, 0.0]),
            f"video_sources.{name}.crop"
        )

        validate_file(file_path)
        duration = probe_duration(file_path, stream_type='v', verbose=verbose)
        width, height = probe_resolution(file_path, verbose=verbose)

        # Calculate sync offset
        # master_time = video_time + offset  =>  video_time = master_time - offset
        offset = master_clap - clap_time

        source_info[name] = {
            'file': file_path,
            'clap_time': clap_time,
            'duration': duration,
            'width': width,
            'height': height,
            'offset': offset,
            'default_z': default_z,
            'timeline': src.get('timeline', []),
            'position_x_pct': pos_x_pct,
            'position_y_pct': pos_y_pct,
            'scale_pct': scale_pct,
            'crop_left_pct': crop_left_pct,
            'crop_right_pct': crop_right_pct,
            'crop_top_pct': crop_top_pct,
            'crop_bottom_pct': crop_bottom_pct
        }

        print(f"  [{name}] {Path(file_path).name}")
        print(f"    Resolution: {width}x{height}")
        print(f"    Duration: {duration:.3f}s")
        print(f"    Clap at: {clap_time:.3f}s")
        print(f"    Sync offset: {offset:+.3f}s")
        print(f"    Default z_index: {default_z:g}")
        print(f"    Placement: x={pos_x_pct:.1f}%, y={pos_y_pct:.1f}%, scale={scale_pct:.1f}%")
        print(f"    Crop: l={crop_left_pct:.1f}%, r={crop_right_pct:.1f}%, "
              f"t={crop_top_pct:.1f}%, b={crop_bottom_pct:.1f}%")
        if 'timeline' in src and src['timeline']:
            print(f"    Timeline events: {len(src['timeline'])}")

    # Production parameters
    production = config['production']
    output_file = production['output_file']
    start = float(production['start'])
    end = float(production['end'])
    output_width = int(production.get('width', 1920))
    output_height = int(production.get('height', 1080))

    production_start = start
    production_end = end
    production_duration = production_end - production_start

    if production_duration <= 0:
        raise AVEditorError(
            f"Invalid production range: start={start}, end={end} "
            f"(duration would be {production_duration:.3f}s)")

    output_path = Path(output_file)

    # Includes (required)
    includes_obj = production.get('includes')
    include_segments = parse_includes(
        includes_obj,
        production_start=production_start,
        production_end=production_end,
        verbose=verbose
    )
    total_duration = sum(seg['end'] - seg['start'] for seg in include_segments)

    print(f"\nProduction:")
    print(f"  Window start in master audio: {production_start:.3f}s")
    print(f"  Window end in master audio:   {production_end:.3f}s")
    print(f"  Window duration:             {production_duration:.3f}s")
    print(f"  Resolution: {output_width}x{output_height}")
    print(f"  Output file: {output_file}")

    if len(include_segments) == 1 and include_segments[0]['start'] == production_start and include_segments[0]['end'] == production_end:
        print(f"  Includes: full window (production.includes is empty)")
    else:
        print(f"  Includes: {len(include_segments)} segment(s), total output duration {total_duration:.3f}s")
        for i, seg in enumerate(include_segments, 1):
            label = seg['label'] if seg['label'] is not None else ''
            print(f"    {i:2d}. {seg['start']:.3f}s - {seg['end']:.3f}s  "
                  f"({seg['end'] - seg['start']:.3f}s) {label}")

    # Check for overwrite
    if output_path.exists() and not args.force:
        raise AVEditorError(
            f"Output file exists: {output_file}\nUse --force to overwrite")

    # Build ffmpeg command
    cmd: List[str] = ['ffmpeg']

    # Enable NVIDIA hardware acceleration for decoding
    # Using 'auto' format so filters work normally, then we encode with NVENC
    cmd.extend(['-hwaccel', 'cuda'])

    filter_parts: List[str] = []
    seg_video_labels: List[str] = []
    seg_audio_labels: List[str] = []

    # We'll create a separate "mini render" per included segment, then concat them.
    input_index = 0
    for seg_idx, seg in enumerate(include_segments):
        seg_start = float(seg['start'])
        seg_end = float(seg['end'])
        seg_duration = seg_end - seg_start

        if verbose:
            print(f"\n[INFO] Segment {seg_idx}: master {seg_start:.3f}s - {seg_end:.3f}s ({seg_duration:.3f}s)")

        # Inputs for this segment
        seg_input_map: Dict[str, int] = {}
        seg_delays: Dict[str, float] = {}         # name -> start pad duration when we can't seek negative
        seg_source_order: Dict[str, int] = {}     # name -> stable tie-breaker for equal z_index

        # Add all video inputs for this segment
        for source_ord, (name, info) in enumerate(source_info.items()):
            seek = seg_start - info['offset']  # desired video timestamp at segment start
            delay = 0.0
            if seek > 0:
                cmd.extend(['-ss', f'{seek:.3f}'])
            else:
                delay = -seek

            # Limit decode to segment duration
            cmd.extend(['-t', f'{seg_duration:.3f}'])
            cmd.extend(['-i', str(info['file'])])

            seg_input_map[name] = input_index
            seg_delays[name] = delay
            seg_source_order[name] = source_ord
            input_index += 1

        # Add audio input for this segment (master audio)
        if seg_start > 0:
            cmd.extend(['-ss', f'{seg_start:.3f}'])
        cmd.extend(['-t', f'{seg_duration:.3f}'])
        cmd.extend(['-i', str(master_file)])
        seg_audio_idx = input_index
        input_index += 1

        # Build per-source state timeline for this segment.
        state_timeline = build_source_state_timeline(video_sources, seg_start, seg_duration)
        state_segments = _state_segments_from_timeline(state_timeline, seg_duration)

        if verbose:
            print(f"[INFO] Source state timeline (segment {seg_idx}):")
            for ts, state in state_timeline:
                draw_order = sorted(state.keys(), key=lambda n: (state[n]['z_index'], seg_source_order.get(n, 0)))
                state_text = ', '.join(
                    f"{name}:z={state[name]['z_index']:g},x={state[name]['position_x_pct']:.1f},"
                    f"y={state[name]['position_y_pct']:.1f},s={state[name]['scale_pct']:.1f},"
                    f"c={state[name]['crop_left_pct']:.1f}/{state[name]['crop_right_pct']:.1f}/"
                    f"{state[name]['crop_top_pct']:.1f}/{state[name]['crop_bottom_pct']:.1f}"
                    for name in draw_order
                )
                print(f"  {ts:6.2f}s: {state_text}")

        if verbose:
            print(f"[INFO] Layered z-state segments (segment {seg_idx}):")
            for s in state_segments:
                st = float(s['start'])
                en = float(s['end'])
                state = s['state']
                draw_order = sorted(state.keys(), key=lambda n: (state[n]['z_index'], seg_source_order.get(n, 0)))
                z_text = ', '.join(f"{name}={state[name]['z_index']:g}" for name in draw_order)
                print(f"  {st:6.2f}s - {en:6.2f}s: {z_text}")

        # Build per-state layered video clips, then concat into this include-segment video.
        state_video_labels: List[str] = []
        for state_idx, state_seg in enumerate(state_segments):
            state_start = float(state_seg['start'])
            state_end = float(state_seg['end'])
            state_duration = state_end - state_start
            state = state_seg['state']

            bg_label = f"seg{seg_idx}_st{state_idx}_bg"
            filter_parts.append(
                f"color=c=black:s={output_width}x{output_height}:d={state_duration:.3f}[{bg_label}]"
            )

            draw_order = sorted(state.keys(), key=lambda n: (state[n]['z_index'], seg_source_order.get(n, 0)))
            if not draw_order:
                state_out = f"seg{seg_idx}_st{state_idx}_v"
                filter_parts.append(f"[{bg_label}]format=yuv420p,setpts=PTS-STARTPTS[{state_out}]")
                state_video_labels.append(state_out)
                continue

            prev_label = bg_label
            for layer_idx, name in enumerate(draw_order):
                safe_name = _safe_label(name)
                clip_label = f"seg{seg_idx}_st{state_idx}_{safe_name}_clip"
                out_label = (
                    f"seg{seg_idx}_st{state_idx}_out"
                    if layer_idx == len(draw_order) - 1
                    else f"seg{seg_idx}_st{state_idx}_layer{layer_idx}"
                )

                layer_state = state[name]
                target_w = max(1, int(round(output_width * (layer_state['scale_pct'] / 100.0))))
                target_h = max(1, int(round(output_height * (layer_state['scale_pct'] / 100.0))))
                delay = seg_delays.get(name, 0.0)
                crop_left_px = int(target_w * (layer_state['crop_left_pct'] / 100.0))
                crop_right_px = int(target_w * (layer_state['crop_right_pct'] / 100.0))
                crop_top_px = int(target_h * (layer_state['crop_top_pct'] / 100.0))
                crop_bottom_px = int(target_h * (layer_state['crop_bottom_pct'] / 100.0))
                crop_w = max(1, target_w - crop_left_px - crop_right_px)
                crop_h = max(1, target_h - crop_top_px - crop_bottom_px)

                # Important: crop is applied after resize so crop never stretches content.
                chain = f"[{seg_input_map[name]}:v]scale={target_w}:{target_h},setsar=1"
                if delay > 0.0005:
                    chain += f",tpad=start_duration={delay:.3f}:start_mode=add"
                chain += f",crop=w={crop_w}:h={crop_h}:x={crop_left_px}:y={crop_top_px}"
                chain += (
                    f",trim=start={state_start:.3f}:end={state_end:.3f},"
                    f"setpts=PTS-STARTPTS[{clip_label}]"
                )
                filter_parts.append(chain)

                overlay_x = int(round(output_width * (layer_state['position_x_pct'] / 100.0))) + crop_left_px
                overlay_y = int(round(output_height * (layer_state['position_y_pct'] / 100.0))) + crop_top_px
                filter_parts.append(
                    f"[{prev_label}][{clip_label}]overlay={overlay_x}:{overlay_y}[{out_label}]"
                )
                prev_label = out_label

            state_out = f"seg{seg_idx}_st{state_idx}_v"
            filter_parts.append(f"[{prev_label}]format=yuv420p,setpts=PTS-STARTPTS[{state_out}]")
            state_video_labels.append(state_out)

        if len(state_video_labels) > 1:
            concat_inputs = ''.join(f"[{label}]" for label in state_video_labels)
            seg_video = f"seg{seg_idx}_vcat"
            filter_parts.append(
                f"{concat_inputs}concat=n={len(state_video_labels)}:v=1:a=0[{seg_video}]"
            )
        else:
            seg_video = state_video_labels[0]

        v_label = f"v{seg_idx}"
        filter_parts.append(f"[{seg_video}]setpts=PTS-STARTPTS[{v_label}]")
        seg_video_labels.append(v_label)

        a_label = f"a{seg_idx}"
        filter_parts.append(
            f"[{seg_audio_idx}:a]atrim=0:{seg_duration:.3f},asetpts=PTS-STARTPTS[{a_label}]"
        )
        seg_audio_labels.append(a_label)

    # Concat segments if needed
    if len(include_segments) > 1:
        concat_in = ''.join(f"[{v}][{a}]" for v, a in zip(seg_video_labels, seg_audio_labels))
        filter_parts.append(
            f"{concat_in}concat=n={len(include_segments)}:v=1:a=1[vout][aout]"
        )
        final_v = "vout"
        final_a = "aout"
    else:
        final_v = seg_video_labels[0]
        final_a = seg_audio_labels[0]

    filter_complex = ';'.join(filter_parts)

    if not ffmpeg_has_encoder('h264_nvenc'):
        raise AVEditorError(
            "This av-editor build is configured to always use NVIDIA NVENC, "
            "but ffmpeg does not expose h264_nvenc. Your ffmpeg may not be compiled with NVENC support."
        )

    print("\nðŸš€ GPU Acceleration enabled:")
    print("  â€¢ Hardware decode: CUDA")
    print("  â€¢ Hardware encode: NVENC (h264_nvenc)")

    # Use NVIDIA NVENC hardware encoding
    # Decode is hardware-accelerated via CUDA, encode via NVENC
    video_codec_args: List[str] = [
        '-c:v', 'h264_nvenc',
        '-preset', 'p4',           # p4 = medium quality preset for newer NVENC
        '-cq', '19',               # constant quality mode
        '-b:v', '0',               # let CQ mode control bitrate
        '-pix_fmt', 'yuv420p',
    ]

    cmd.extend([
        '-filter_complex', filter_complex,
        '-map', f'[{final_v}]',
        '-map', f'[{final_a}]',
    ])
    cmd.extend(video_codec_args)
    cmd.extend([
        '-c:a', 'aac',
        '-b:a', '192k',
        '-movflags', '+faststart',
    ])

    # For atomic writes, use temp file
    if not args.dry_run:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        temp_fd, temp_path = tempfile.mkstemp(
            suffix=output_path.suffix,
            dir=output_path.parent,
            prefix='.av-editor-tmp-'
        )
        os.close(temp_fd)
        temp_path = Path(temp_path)
        os.chmod(temp_path, 0o644)
        target = temp_path
    else:
        target = output_path

    cmd.extend(['-y', str(target)])

    print()
    try:
        run_command(cmd, verbose=verbose, dry_run=args.dry_run)

        if not args.dry_run:
            temp_path.rename(output_path)
            print(f"\nâœ“ Rendered: {output_path}")
    except Exception:
        if not args.dry_run and temp_path.exists():
            temp_path.unlink()
        raise


def main():
    parser = argparse.ArgumentParser(
        prog='av-editor',
        description='Multi-source timeline editor with clapboard sync'
    )
    parser.add_argument('--version', action='version',
                        version=f'av-editor {VERSION}')

    subparsers = parser.add_subparsers(dest='command')

    render_parser = subparsers.add_parser(
        'render',
        help='Render synchronized output from a config file'
    )
    render_parser.add_argument('config', help='JSON config file')
    render_parser.add_argument('-v', '--verbose', action='store_true',
                               help='Show ffmpeg commands and sync details')
    render_parser.add_argument('--dry-run', action='store_true',
                               help='Show what would happen without executing')
    render_parser.add_argument('--force', action='store_true',
                               help='Overwrite output file if it exists')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return 0

    try:
        if args.command == 'render':
            render(args)
        return 0
    except AVEditorError as e:
        print(f"ERROR: {e}", file=sys.stderr)
        return 1
    except KeyboardInterrupt:
        print("\nInterrupted", file=sys.stderr)
        return 1
    except Exception as e:
        print(f"UNEXPECTED ERROR: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
